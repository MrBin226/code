开发者工具
    - Ctrl+F (在当前面板搜索)
    - Ctrl+shift+F (在全局搜索)

scrapy持久化存储
    - 基于终端指令：scrapy crawl qiushi -o ./qiushi.csv
        - 要求：只可以将parse方法的返回值存储到本地的文本文件中
        - 注意：持久化存储对应的文件类型只可以是如下：
          '.json', '.jsonlines', '.jl', '.csv', '.xml', '.marshal', '.pickle'

    - 基于管道：
        - 编码流程：
            - 数据解析
            - 在item类中定义相关的属性
            - 将解析的数据封装存储到item类型的对象中
            - 将item类型的对象提交给管道进行持久化存储的操作
            - 在管道内的process_item中要将其接受到的item对象中存储的数据进行持久化存储操作
            - 在配置文件中开启管道

基于spider的全站数据爬取
    - 就是将网站中某板块下的全部页码对应的页面数据进行爬取
    - 爬取校花网上的照片名称
    - 实现方式：手动发送请求
        yield scrapy.Request(url=new_url, callback=self.parse)

scrapy五大核心组件简介
    - 引擎(Scrapy)
        用来处理整个系统的数据流处理, 触发事务(框架核心)
    - 调度器(Scheduler)
        用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回.
        可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么,
        同时去除重复的网址
    - 下载器(Downloader)
        用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)
    - 爬虫(Spiders)
        爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。
        用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
    - 项目管道(Pipeline)
        负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。
        当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。


scrapy的请求传参
    - 使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）
    - 需求：爬取boss的岗位名称，岗位描述
        - 使用meta={},传输item,使用yield scrapy.Request(url=new_url, callback=self.parse_deail, meta={})
        使用response.meta[]获取传输的item


图片数据爬取之ImagesPipeline
    - 基于scrapy爬取字符串类型的数据和爬取图片类型的数据区别？
        - 字符串：只需要基于xpath进行解析并提交管道进行持久化存储
        - 图片： xpath解析出图片scr的属性值，单独对图片地址发起请求获取图片二进制类型的数据
    - ImagesPipeline:
        - 只需要将img的src的属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制数据，且还会帮我们进行持久化存储
        - 需求：爬取站长素材的高清图片
        - 使用流程：
            - 数据解析（图片的地址）
            - 将存储图片地址的item提交到制定的管道类
            - 在管道文件中自定制一个基于ImagesPipeline的一个管道类，可重写如下3个方法：
                - get_media_requests()
                - file_path()
                - item_completed()
            - 在配置文件中进行如下配置：IMAGES_STORE = ‘./imgs’：表示最终图片存储的目录

下载中间件
    - 位置：引擎和下载器之间，在middlewares.py文件里
    - 作用：批量拦截到整个工程中所有的请求和响应
    - 拦截请求：
        - UA伪装：process_request-->return request
        - 代理IP：process_exception-->return request
    - 拦截响应：
        - 篡改响应数据、响应对象：process_response-->return response
        - 需求：爬取网易新闻中的新闻数据（标题和内容）
            - 1、通过网易新闻中的首页解析出五大板块所对应的详情页的url
            - 2、每一个板块对应的新闻标题都是动态加载出来的
            - 3、通过解析出每一条新闻详情页的url获取详情页的页面源码，解析出新闻具体内容

CrawlSpider类，Spider的一个子类
    - 全站数据爬取
    - CrawlSpider的使用：
        - 创建一个工程
        - 创建爬虫文件（CrawlSpider）
            - scrapy genspider -t crawl xxx xxx.com
            - 链接提取器：link = LinkExtractor(allow=r'id=1&page=\d+')
                - 根据指定规则（allow=“正则表达式”）进行指定链接的提取
            - 规则解析器：rules = (Rule(link, callback='parse_item', follow=True),)
                - 将链接提取器提取到的链接进行指定规则（callback）的解析
                - follow=True,可以将链接提取器 继续作用到 链接提取器提取到的链接 所对应的页面中
